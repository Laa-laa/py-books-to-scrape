import script_function
import os
import random
import string
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import csv

# Fonction pour télécharger une image à partir d'une URL et la sauvegarder localement
def download_image(image_url, category_name):
    response = requests.get(image_url, stream=True)
    if response.status_code == 200:
        # Générer un nom de fichier aléatoire
        random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
        filename = f"{category_name}_{random_str}.png"
        # Créer le dossier "/images" s'il n'existe pas
        os.makedirs("images", exist_ok=True)
        # Sauvegarder l'image dans le dossier "/images" avec le nom généré
        filepath = os.path.join("images", filename)
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        print(f"Image téléchargée : {filename}")
    else:
        print(f"Erreur lors du téléchargement de l'image {image_url} :", response.status_code)

# Fonction principale pour scraper tous les livres et télécharger les images de couverture
def scrape_all_books(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        categories = soup.find('ul', class_='nav-list').find_all('a')
        for category in categories:
            category_url = urljoin(url, category['href'])
            category_name = category.text.strip()
            if category_name == "Books":
                print("Skipping 'Books' category...")
                continue
            print(f"Scraping category: {category_name}")
            category_books_info = script_function.scrape_category_books(category_url)
            for book_info in category_books_info:
                image_url = urljoin(url, book_info['image_url'])
                download_image(image_url, category_name)
            save_to_csv(category_books_info, category_name)
            print(f"Category {category_name} scraped and saved successfully.")
    else:
        print("Erreur lors de la requête HTTP :", response.status_code)

def save_to_csv(data, category_name):
    cleaned_data = script_function.clean_data(data)
    filename = f"{category_name}_books_info.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=cleaned_data[0].keys())
        writer.writeheader()
        writer.writerows(cleaned_data)

# Exemple d'utilisation
if __name__ == "__main__":
    scrape_all_books("https://books.toscrape.com")
